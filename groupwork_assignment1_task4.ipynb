{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. Empirical Analysis of ETFs\n",
    "Pick a sector ETF (in the US, for example, XLRE)\n",
    "\n",
    "a. Find the 30 largest holdings.\n",
    "\n",
    "b. Get at least 6 months of data (~ 120 data points).\n",
    "\n",
    "c. Compute the daily returns.\n",
    "\n",
    "d. Compute the covariance matrix.\n",
    "\n",
    "e. Compute the PCA.\n",
    "\n",
    "f. Compute the SVD."
   ],
   "id": "a95d933d0964419d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# XLU empirical workflow with robust holdings parsing (English-only comments)\n",
    "# Steps:\n",
    "# (a) Top-30 holdings (robust to header changes; fallback if no tickers)\n",
    "# (b) >= ~6 months of daily prices\n",
    "# (c) Daily returns\n",
    "# (d) Covariance matrix\n",
    "# (e) PCA on standardized returns\n",
    "# (f) SVD of standardized returns matrix\n",
    "\n",
    "import io\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "FUND = \"XLU\"\n",
    "SSGA_XLSX = \"https://www.ssga.com/library-content/products/fund-data/etfs/us/holdings-daily-us-en-xlu.xlsx\"\n",
    "ALT_HOLDINGS_HTML = \"https://stockanalysis.com/etf/xlu/holdings/\"  # fallback table\n",
    "END = datetime.today().date()\n",
    "START = END - timedelta(days=280)         # ~9 calendar months to ensure >= ~6 months trading days\n",
    "TARGET_TRADING_DAYS = 126                 # ~6 months of trading days\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def _first_matching_col(columns, patterns, case_insensitive=True):\n",
    "    \"\"\"\n",
    "    Return the first column whose name matches any of the regex patterns.\n",
    "    \"\"\"\n",
    "    flags = re.IGNORECASE if case_insensitive else 0\n",
    "    for pat in patterns:\n",
    "        for c in columns:\n",
    "            if re.search(pat, str(c), flags):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def _numeric_percent_series(s):\n",
    "    \"\"\"\n",
    "    Convert a column that may contain strings like '3.45%' into float 3.45.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.replace(\",\", \"\", regex=False)\n",
    "         .str.extract(r\"([-+]?\\d*\\.?\\d+)\")\n",
    "         .iloc[:, 0]\n",
    "         .astype(float)\n",
    "    )\n",
    "\n",
    "def _normalize_ticker(s):\n",
    "    \"\"\"\n",
    "    Normalize tickers for Yahoo Finance compatibility (common cases).\n",
    "    \"\"\"\n",
    "    return (s.astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace(\"/\", \"-\", regex=False)   # e.g., BRK/B -> BRK-B\n",
    "            .str.replace(\" \", \"\", regex=False)\n",
    "           )\n",
    "\n",
    "# -----------------------------\n",
    "# (a) Robust holdings fetcher\n",
    "# -----------------------------\n",
    "def get_xlu_holdings_robust():\n",
    "    \"\"\"\n",
    "    Try SSGA XLSX first. Fuzzy-detect Name/Ticker/Weight.\n",
    "    If no Ticker column can be found, fall back to StockAnalysis HTML table\n",
    "    and merge tickers by company name.\n",
    "    Returns a DataFrame with columns: ['Name', 'Ticker', 'Weight'].\n",
    "    \"\"\"\n",
    "    # 1) Try SSGA XLSX\n",
    "    primary_source = \"SSGA XLSX\"\n",
    "    try:\n",
    "        r = requests.get(SSGA_XLSX, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        df = pd.read_excel(io.BytesIO(r.content), engine=\"openpyxl\")\n",
    "        raw_cols = list(df.columns)\n",
    "\n",
    "        # Heuristics for column names that often change\n",
    "        name_col = _first_matching_col(\n",
    "            raw_cols,\n",
    "            patterns=[r\"^name$\", r\"security.*name\", r\"company\", r\"issuer\", r\"holding.*name\"]\n",
    "        )\n",
    "        ticker_col = _first_matching_col(\n",
    "            raw_cols,\n",
    "            patterns=[r\"ticker\", r\"symbol\", r\"trading.*symbol\", r\"local.*ticker\", r\"bbg.*ticker\"]\n",
    "        )\n",
    "        weight_col = _first_matching_col(\n",
    "            raw_cols,\n",
    "            patterns=[r\"weight\", r\"% of\", r\"percent\"]\n",
    "        )\n",
    "\n",
    "        print(f\"[INFO] SSGA columns detected: name={name_col}, ticker={ticker_col}, weight={weight_col}\")\n",
    "        if name_col is None or weight_col is None:\n",
    "            raise ValueError(\"Could not detect required columns from SSGA XLSX.\")\n",
    "\n",
    "        out = pd.DataFrame()\n",
    "        out[\"Name\"] = df[name_col].astype(str).str.strip()\n",
    "\n",
    "        # Parse weight into numeric percentage (not fraction)\n",
    "        out[\"Weight\"] = _numeric_percent_series(df[weight_col])\n",
    "\n",
    "        if ticker_col is not None:\n",
    "            out[\"Ticker\"] = _normalize_ticker(df[ticker_col])\n",
    "            # Drop empty tickers\n",
    "            out = out.dropna(subset=[\"Ticker\"])\n",
    "            out = out[out[\"Ticker\"].str.len() > 0]\n",
    "            # Primary case: we have tickers\n",
    "            print(\"[INFO] Using SSGA tickers.\")\n",
    "            return out[[\"Name\", \"Ticker\", \"Weight\"]].dropna(subset=[\"Weight\"])\n",
    "        else:\n",
    "            # 2) Fallback to alternate source for tickers\n",
    "            print(\"[WARN] No ticker column in SSGA XLSX. Falling back to StockAnalysis for tickers...\")\n",
    "            alt_tables = pd.read_html(ALT_HOLDINGS_HTML)\n",
    "            # The first or second table usually contains Name/Symbol/% Weight columns\n",
    "            alt = None\n",
    "            for t in alt_tables:\n",
    "                cols_lower = [c.lower() for c in t.columns]\n",
    "                if any((\"name\" in c or \"company\" in c) for c in cols_lower) and any((\"symbol\" in c or \"ticker\" in c) for c in cols_lower):\n",
    "                    alt = t.copy()\n",
    "                    break\n",
    "            if alt is None:\n",
    "                raise ValueError(\"Fallback holdings table not found on StockAnalysis.\")\n",
    "\n",
    "            # Standardize fallback columns\n",
    "            # Try to identify columns by fuzzy match\n",
    "            alt_name_col = _first_matching_col(list(alt.columns), [r\"name\", r\"company\"])\n",
    "            alt_ticker_col = _first_matching_col(list(alt.columns), [r\"symbol\", r\"ticker\"])\n",
    "            alt_weight_col = _first_matching_col(list(alt.columns), [r\"weight\", r\"%\"])\n",
    "            if alt_weight_col is not None:\n",
    "                alt[\"Weight_fallback\"] = _numeric_percent_series(alt[alt_weight_col])\n",
    "            else:\n",
    "                alt[\"Weight_fallback\"] = np.nan\n",
    "\n",
    "            alt = alt.rename(columns={alt_name_col: \"Name_fallback\", alt_ticker_col: \"Ticker\"})\n",
    "            alt[\"Name_fallback\"] = alt[\"Name_fallback\"].astype(str).str.strip()\n",
    "            alt[\"Ticker\"] = _normalize_ticker(alt[\"Ticker\"])\n",
    "\n",
    "            # Merge by company name (case-insensitive)\n",
    "            merged = pd.merge(\n",
    "                out,\n",
    "                alt[[\"Name_fallback\", \"Ticker\"]],\n",
    "                left_on=out[\"Name\"].str.lower(),\n",
    "                right_on=alt[\"Name_fallback\"].str.lower(),\n",
    "                how=\"left\",\n",
    "                suffixes=(\"\",\"\")\n",
    "            ).drop(columns=[\"key_0\", \"Name_fallback\"])\n",
    "\n",
    "            # If still missing tickers for a few rows, drop them (rare)\n",
    "            missing_before = merged[\"Ticker\"].isna().sum()\n",
    "            if missing_before > 0:\n",
    "                warnings.warn(f\"{missing_before} rows had no ticker match; they will be dropped.\")\n",
    "            merged = merged.dropna(subset=[\"Ticker\"])\n",
    "\n",
    "            print(\"[INFO] Using fallback tickers from StockAnalysis.\")\n",
    "            return merged[[\"Name\", \"Ticker\", \"Weight\"]].dropna(subset=[\"Weight\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Primary holdings fetch failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_top30_xlu():\n",
    "    holdings = get_xlu_holdings_robust()\n",
    "    # Take top-30 by weight\n",
    "    top30 = (holdings.sort_values(\"Weight\", ascending=False)\n",
    "                      .head(30)\n",
    "                      .reset_index(drop=True))\n",
    "    return top30\n",
    "\n",
    "# -----------------------------\n",
    "# Run (a) holdings\n",
    "# -----------------------------\n",
    "top30 = get_top30_xlu()\n",
    "print(\"\\n=== (a) XLU Top 30 Holdings ===\")\n",
    "print(top30)\n",
    "\n",
    "# -----------------------------\n",
    "# (b) >= 6 months of daily prices for XLU + top-30\n",
    "# -----------------------------\n",
    "tickers = [FUND] + top30[\"Ticker\"].dropna().astype(str).unique().tolist()\n",
    "prices = yf.download(tickers, start=START, end=END, auto_adjust=True, progress=False)[\"Close\"]\n",
    "\n",
    "# Keep last ~126 trading days and drop assets with any missing values in this window\n",
    "prices = prices.tail(TARGET_TRADING_DAYS).dropna(axis=1, how=\"any\")\n",
    "\n",
    "print(f\"\\n=== (b) Prices shape: {prices.shape} (rows ~ trading days, cols ~ symbols) ===\")\n",
    "print(prices.tail(3))\n",
    "\n",
    "# -----------------------------\n",
    "# (c) Daily returns\n",
    "# -----------------------------\n",
    "rets = prices.pct_change().dropna()\n",
    "print(f\"\\n=== (c) Daily returns shape: {rets.shape} ===\")\n",
    "print(rets.head())\n",
    "\n",
    "# -----------------------------\n",
    "# (d) Covariance matrix\n",
    "# -----------------------------\n",
    "cov = rets.cov()\n",
    "print(\"\\n=== (d) Covariance matrix (head) ===\")\n",
    "print(cov.iloc[:10, :10])\n",
    "\n",
    "# -----------------------------\n",
    "# (e) PCA on standardized returns (correlation structure)\n",
    "# -----------------------------\n",
    "rets_std = (rets - rets.mean()) / rets.std(ddof=1)\n",
    "ncomp = min(10, rets_std.shape[1])\n",
    "pca = PCA(n_components=ncomp).fit(rets_std)\n",
    "\n",
    "evr = pd.Series(pca.explained_variance_ratio_, index=[f\"PC{i+1}\" for i in range(ncomp)])\n",
    "loadings = pd.DataFrame(pca.components_, columns=rets_std.columns, index=evr.index)\n",
    "\n",
    "print(\"\\n=== (e) PCA explained variance ratio ===\")\n",
    "print(evr.round(4))\n",
    "\n",
    "print(\"\\n=== PCA loadings (first 3 PCs, first 12 tickers) ===\")\n",
    "print(loadings.iloc[:3, :12].round(3))\n",
    "\n",
    "# -----------------------------\n",
    "# (f) SVD of standardized returns matrix\n",
    "# -----------------------------\n",
    "X = rets_std.values  # shape T x N\n",
    "U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "print(\"\\n=== (f) SVD singular values (first 10) ===\")\n",
    "print(np.round(s[:10], 6))\n",
    "\n",
    "print(\"\\n=== Right singular vectors V^T (first 3 rows, first 12 tickers) ===\")\n",
    "print(pd.DataFrame(Vt[:3, :12], columns=rets_std.columns[:12], index=[\"SV1\",\"SV2\",\"SV3\"]).round(3))\n"
   ],
   "id": "969c13b114e85255"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
